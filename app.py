import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
from datetime import datetime
import io

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Qoo10.jp ìƒí’ˆ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ›ï¸",
    layout="wide"
)

class Qoo10Crawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_page_content(self, url):
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            st.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_product_info(self, item_element, rank):
        """ê°œë³„ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ê´‘ê³  ìƒí’ˆ ì²´í¬ (PR íƒœê·¸ê°€ ìˆìœ¼ë©´ ì œì™¸)
            ad_tag = item_element.find('span', class_='ad_cps')
            if ad_tag and ad_tag.get_text(strip=True) == 'PR':
                return None
            
            # ë¸Œëœë“œëª… ì¶”ì¶œ
            brand_element = item_element.find('a', class_='txt_brand')
            brand_name = ''
            if brand_element:
                brand_name = brand_element.get_text(strip=True)
                brand_name = re.sub(r'^å…¬å¼\s*', '', brand_name)
            
            # ì œí’ˆëª… ì¶”ì¶œ
            title_element = item_element.find('a', class_='tt')
            product_name = ''
            product_link = ''
            if title_element:
                product_name = title_element.get('title', '').strip()
                if not product_name:
                    product_name = title_element.get_text(strip=True)
                product_link = title_element.get('href', '')
                if product_link and not product_link.startswith('http'):
                    product_link = urljoin('https://www.qoo10.jp', product_link)
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ
            price_section = item_element.find('div', class_='prc')
            original_price = ''
            sale_price = ''
            
            if price_section:
                del_element = price_section.find('del')
                if del_element:
                    original_price = del_element.get_text(strip=True)
                
                strong_element = price_section.find('strong')
                if strong_element:
                    sale_price = strong_element.get_text(strip=True)
            
            # ë¦¬ë·° ìˆ˜ ì¶”ì¶œ
            review_count = ''
            review_element = item_element.find('span', class_='review_total_count')
            if review_element:
                review_text = review_element.get_text(strip=True)
                review_match = re.search(r'\((\d+)\)', review_text)
                if review_match:
                    review_count = review_match.group(1)
            
            return {
                'ìˆœìœ„': rank,
                'ë¸Œëœë“œëª…': brand_name,
                'ì œí’ˆëª…': product_name,
                'ì •ê°€': original_price,
                'íŒë§¤ê°€': sale_price,
                'ì œí’ˆë§í¬': product_link,
                'ë¦¬ë·°ìˆ˜': review_count
            }
        
        except Exception as e:
            st.error(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def crawl_products(self, url, progress_bar=None, status_text=None):
        """ìƒí’ˆ ëª©ë¡ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        if status_text:
            status_text.text("í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        html_content = self.get_page_content(url)
        if not html_content:
            return []
        
        soup = BeautifulSoup(html_content, 'html.parser')
        item_elements = soup.find_all('div', class_='item')
        
        if not item_elements:
            st.warning("ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return []
        
        if status_text:
            status_text.text(f"ì´ {len(item_elements)}ê°œì˜ ìƒí’ˆì„ ì²˜ë¦¬ ì¤‘...")
        
        products = []
        rank = 1
        
        for i, item in enumerate(item_elements):
            if progress_bar:
                progress_bar.progress((i + 1) / len(item_elements))
            
            product_info = self.extract_product_info(item, rank)
            if product_info:
                products.append(product_info)
                rank += 1
            
            time.sleep(0.1)
        
        return products

def convert_df_to_csv(df):
    """DataFrameì„ CSVë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    output = io.StringIO()
    df.to_csv(output, index=False, encoding='utf-8')
    return output.getvalue().encode('utf-8-sig')

def main():
    st.title("ğŸ›ï¸ Qoo10.jp ìƒí’ˆ í¬ë¡¤ëŸ¬")
    st.markdown("Qoo10.jpì˜ ìƒí’ˆ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ì—¬ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ“‹ ì‚¬ìš©ë²•")
    st.sidebar.markdown("""
    1. Qoo10.jp ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ URLì„ ì…ë ¥
    2. 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ í´ë¦­
    3. ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  CSV ë‹¤ìš´ë¡œë“œ
    
    **ì£¼ì˜ì‚¬í•­:**
    - ê´‘ê³  ìƒí’ˆ(PR)ì€ ì œì™¸ë©ë‹ˆë‹¤
    - ë„ˆë¬´ ë¹ˆë²ˆí•œ ìš”ì²­ì€ í”¼í•´ì£¼ì„¸ìš”
    """)
    
    # URL ì…ë ¥
    url = st.text_input(
        "ğŸ”— Qoo10.jp URLì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="https://www.qoo10.jp/cat/120000012/220000159?...",
        help="Qoo10.jpì˜ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    )
    
    # í¬ë¡¤ë§ ë²„íŠ¼
    if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary"):
        if not url:
            st.error("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if 'qoo10.jp' not in url:
            st.error("Qoo10.jp URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler = Qoo10Crawler()
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        with st.spinner("í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤..."):
            products = crawler.crawl_products(url, progress_bar, status_text)
        
        # ê²°ê³¼ ì²˜ë¦¬
        if products:
            df = pd.DataFrame(products)
            
            st.success(f"âœ… ì´ {len(products)}ê°œì˜ ìƒí’ˆì„ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤!")
            
            # ê²°ê³¼ í‘œì‹œ
            st.subheader("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
            st.dataframe(df, use_container_width=True)
            
            # í†µê³„ ì •ë³´
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ìƒí’ˆ ìˆ˜", len(products))
            with col2:
                brands = df['ë¸Œëœë“œëª…'].nunique()
                st.metric("ë¸Œëœë“œ ìˆ˜", brands)
            with col3:
                avg_reviews = df['ë¦¬ë·°ìˆ˜'].replace('', '0').astype(int).mean()
                st.metric("í‰ê·  ë¦¬ë·° ìˆ˜", f"{avg_reviews:.1f}")
            with col4:
                has_discount = df['ì •ê°€'].ne('').sum()
                st.metric("í• ì¸ ìƒí’ˆ ìˆ˜", has_discount)
            
            # CSV ë‹¤ìš´ë¡œë“œ
            csv_data = convert_df_to_csv(df)
            current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"qoo10_products_{current_time}.csv"
            
            st.download_button(
                label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=csv_data,
                file_name=filename,
                mime="text/csv",
                type="primary"
            )
            
        else:
            st.error("âŒ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    st.markdown(
        "â„¹ï¸ ì´ ë„êµ¬ëŠ” ê°œì¸ì ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”. "
        "ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ ì¤€ìˆ˜í•˜ê³  ê³¼ë„í•œ ìš”ì²­ì€ í”¼í•´ì£¼ì„¸ìš”."
    )

if __name__ == "__main__":
    main()